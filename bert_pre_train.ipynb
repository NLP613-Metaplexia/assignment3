{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLP613-Metaplexia/assignment3/blob/main/bert_pre_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxgADpDIIf4_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0a6eb6f0feb24a9d94e4f57f29dd9455"
          ]
        },
        "id": "0ybpPqCrIf5A",
        "outputId": "912c805d-762f-44ca-a5f5-930f5cc879bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a6eb6f0feb24a9d94e4f57f29dd9455",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from prettytable import PrettyTable\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from evaluate import (evaluator, load)\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoConfig,\n",
        "                          AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          EarlyStoppingCallback,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                        )\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-nmrlkMIf5B"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlfMnzYPIf5B"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "context_length = 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXCDF6rLIf5B"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-A2q5HFIf5C"
      },
      "outputs": [],
      "source": [
        "def my_preplexity(model_name):\n",
        "    perplexity = load(\"perplexity\", module_type=\"metric\")\n",
        "    input_texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP # limiting the sample to 10\n",
        "    input_texts = [s for s in input_texts if s!='']\n",
        "    results = perplexity.compute(model_id=model_name,\n",
        "                                predictions=input_texts,\n",
        "                                add_start_token=False)\n",
        "    # Print the mean perplexity value rounded to 2 decimal places\n",
        "    print(round(results[\"mean_perplexity\"], 2)) # doctest: +SKIP\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________________________\n",
        "# > https://stackoverflow.com/questions/68058647/initialize-huggingface-bert-with-random-weights\n",
        "def randomize_model(model):\n",
        "    for module_ in model.named_modules():\n",
        "        if isinstance(module_[1],(torch.nn.Linear, torch.nn.Embedding)):\n",
        "            module_[1].weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
        "        elif isinstance(module_[1], torch.nn.LayerNorm):\n",
        "            module_[1].bias.data.zero_()\n",
        "            module_[1].weight.data.fill_(1.0)\n",
        "        if isinstance(module_[1], torch.nn.Linear) and module_[1].bias is not None:\n",
        "            module_[1].bias.data.zero_()\n",
        "    return model\n",
        "\n",
        "def model_size_and_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    model_size = sum(t.numel() for t in model.parameters())\n",
        "    print(f\"bert-base-uncased size: {model_size/1000**2:.1f}M parameters\")\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "\n",
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        # if length == context_length:\n",
        "        input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yH6hh2AIf5D"
      },
      "source": [
        "# Bert Base Uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9cbcfb502e5a4ef6b1f4c50b1df9a482"
          ]
        },
        "id": "X-ewYh_pIf5D",
        "outputId": "0217904c-5b9c-4ab3-b0a9-a73af79d63c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cbcfb502e5a4ef6b1f4c50b1df9a482",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "738800.38\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "my_preplexity(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wnjfg0VIf5D",
        "outputId": "de98116e-979a-4f4b-b5cb-0405271e19a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert-base-uncased size: 109.5M parameters\n",
            "+---------------------------------------------------------+------------+\n",
            "|                         Modules                         | Parameters |\n",
            "+---------------------------------------------------------+------------+\n",
            "|          bert.embeddings.word_embeddings.weight         |  23440896  |\n",
            "|        bert.embeddings.position_embeddings.weight       |   393216   |\n",
            "|       bert.embeddings.token_type_embeddings.weight      |    1536    |\n",
            "|             bert.embeddings.LayerNorm.weight            |    768     |\n",
            "|              bert.embeddings.LayerNorm.bias             |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.0.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.1.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.2.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.3.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.4.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.5.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.6.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.7.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.8.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |\n",
            "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |\n",
            "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |\n",
            "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |\n",
            "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |\n",
            "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |\n",
            "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |\n",
            "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |\n",
            "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |\n",
            "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |\n",
            "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |\n",
            "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |\n",
            "|          bert.encoder.layer.9.output.dense.bias         |    768     |\n",
            "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |\n",
            "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.10.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |\n",
            "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |\n",
            "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |\n",
            "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |\n",
            "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |\n",
            "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |\n",
            "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |\n",
            "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n",
            "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n",
            "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |\n",
            "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |\n",
            "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |\n",
            "|         bert.encoder.layer.11.output.dense.bias         |    768     |\n",
            "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |\n",
            "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |\n",
            "|                   cls.predictions.bias                  |   30522    |\n",
            "|          cls.predictions.transform.dense.weight         |   589824   |\n",
            "|           cls.predictions.transform.dense.bias          |    768     |\n",
            "|        cls.predictions.transform.LayerNorm.weight       |    768     |\n",
            "|         cls.predictions.transform.LayerNorm.bias        |    768     |\n",
            "+---------------------------------------------------------+------------+\n",
            "Total Trainable Params: 109514298\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "109514298"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_size_and_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWhuirUzIf5D"
      },
      "source": [
        "# randomized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6d735fccc8cb4edebfb00bf7cd1e6734"
          ]
        },
        "id": "kNPy4bpKIf5D",
        "outputId": "ea77db44-e4de-406d-a302-b38b37dda489"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ankityadav/miniconda3/envs/env1_38/lib/python3.8/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/home/ankityadav/miniconda3/envs/env1_38/lib/python3.8/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d735fccc8cb4edebfb00bf7cd1e6734",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31341.62\n"
          ]
        }
      ],
      "source": [
        "rand_model = randomize_model(model)\n",
        "rand_model.push_to_hub(\"rand_model\", use_auth_token=\"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\")\n",
        "tokenizer.push_to_hub(\"rand_model\", use_auth_token=\"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\")\n",
        "\n",
        "my_preplexity(\"temporary0-0name/rand_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiHKM_wMIf5E"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rwTYBEAIf5E",
        "outputId": "2814a94b-166a-4592-ea32-56decea05590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw_datasets DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36718\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n",
            "Tokenized_datasets DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids'],\n",
            "        num_rows: 46621\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['input_ids'],\n",
            "        num_rows: 4783\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "ds_train = load_dataset(\"wikitext\",\"wikitext-2-raw-v1\",split=\"train\")\n",
        "ds_valid = load_dataset(\"wikitext\",\"wikitext-2-raw-v1\",split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "\n",
        "print(f'Raw_datasets',raw_datasets)\n",
        "print(f'Tokenized_datasets',tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnX_fPTqIf5E",
        "outputId": "4bb04c54-d385-494d-a182-de05136bb731"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape: torch.Size([5, 128])\n",
            "attention_mask shape: torch.Size([5, 128])\n",
            "labels shape: torch.Size([5, 128])\n"
          ]
        }
      ],
      "source": [
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trwUQcC4If5E"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq_Hdq99If5F"
      },
      "outputs": [],
      "source": [
        "# Optimal hyperparameters\n",
        "\n",
        "learning_rate, batch, epoch, weigh_decay = (0.0003, 64, 10, 0.1)\n",
        "learning_rate, batch, epoch, weigh_decay = (0.0003, 32, 10, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9e895034d83e403e89731684561f6924",
            "f8e6028700574dab9680fc215474dd83",
            "18c6019592ce4e47afaa8bd203d18294"
          ]
        },
        "id": "PKFxQouwIf5F",
        "outputId": "48826c90-83c4-4134-9fe4-e6df421461eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1820' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1820/1820 35:18, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.625200</td>\n",
              "      <td>6.411256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.839000</td>\n",
              "      <td>2.038549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.913700</td>\n",
              "      <td>0.310766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>0.087743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>0.039603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.024211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.014800</td>\n",
              "      <td>0.017986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.009800</td>\n",
              "      <td>0.014752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.007700</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.012051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.011534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.010980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>0.010855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.010770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.010724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.010705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.010702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e895034d83e403e89731684561f6924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8e6028700574dab9680fc215474dd83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c6019592ce4e47afaa8bd203d18294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.02\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"run_opt\",\n",
        "    per_device_train_batch_size=batch,\n",
        "    per_device_eval_batch_size=batch,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=epoch,\n",
        "    weight_decay=weigh_decay,\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=learning_rate,\n",
        "    save_steps=100,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=rand_model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "trainer.train()\n",
        "trainer.push_to_hub()\n",
        "\n",
        "my_preplexity(\"temporary0-0name/run_opt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LhMKm5aIf5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azirn4rYIf5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GKz1GlgIf5F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env1_38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}